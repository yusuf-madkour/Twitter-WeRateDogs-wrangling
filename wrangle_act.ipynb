{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gather\n",
    "\n",
    "For this project, I will gather data from three different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The first source\n",
    "The `twitter-archive-enhanced.csv` which was provided by *Udacity* for this project.\n",
    "\n",
    "This file contains the twitter archive of the [WeRateDogs twitter account](https://twitter.com/dog_rates?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) which is the \"only source for professional dog ratings\" as they describe themselves on their twitter account.\n",
    "\n",
    "I downloaded this file manually and added it to the folder of the project under its original name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./Slides/Manual_Download_Proof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing downloaded data into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweet_archive_df = pd.read_csv(\"twitter-archive-enhanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_archive_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The second source\n",
    "\n",
    "The `image_predictions.tsv` which is hosted on *Udacity* servers. This file contains the tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downloading the file programmatically using *requests* Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "predictions_file_name = url.rsplit(\"/\")[-1]\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(predictions_file_name, 'wb') as saved_file:\n",
    "    saved_file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./Slides/Programmatic_Download_Proof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing the collected data in a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_pred_df = pd.read_csv(predictions_file_name, sep=\"\\t\")\n",
    "img_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The third source\n",
    "\n",
    "Using the `tweet_id` column in the WeRateDogs Twitter archive `twitter-archive-enhanced.csv`, I will query the Twitter API for each tweet's JSON data using Python's *Tweepy* module and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet's JSON data will be written to its own line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading secret API keys from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "CONSUMER_KEY = os.getenv('CONSUMER_KEY')\n",
    "CONSUMER_SECRET = os.getenv('CONSUMER_SECRET')\n",
    "ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')\n",
    "ACCESS_SECRET = os.getenv('ACCESS_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Querying the Twitter API through Tweepy to collect tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "# Authentication\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                 wait_on_rate_limit_notify=True)\n",
    "\n",
    "tweet_ids = pd.read_csv(\"twitter-archive-enhanced.csv\").tweet_id\n",
    "\n",
    "with open(\"tweet-json.txt\", \"w\") as txt_file:\n",
    "    # For every tweet ID\n",
    "    for tweet_id in tweet_ids:\n",
    "        # Try to get the tweet information\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode=\"extended\")\n",
    "        except tweepy.TweepError:\n",
    "            continue\n",
    "\n",
    "        # If successful, write the json string to the tweet-json.txt file and append a new line\n",
    "        tweet_json_string = json.dumps(tweet)\n",
    "        txt_file.write(f\"{tweet_json_string}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Slides/API_Querying_proof.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving collected tweets data into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_df = pd.DataFrame()\n",
    "with open('tweet-json.txt', 'r') as txt_file:\n",
    "    tweet_data_df = pd.read_json(txt_file, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "In this section, I gathered data from 3 different sources and saved them to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dogs]",
   "language": "python",
   "name": "conda-env-dogs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
